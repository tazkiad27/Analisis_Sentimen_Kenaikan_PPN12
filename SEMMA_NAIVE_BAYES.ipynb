{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1rn30vSEoWl"
      },
      "source": [
        "# SAMPLE (Scraping Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFAaAAIpeTk"
      },
      "outputs": [],
      "source": [
        "twitter_auth_token = '90d'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xizVIYDwzuu"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" |sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uVkzZmQxno_"
      },
      "outputs": [],
      "source": [
        "# Crawl Data\n",
        "filename = 'ppn.csv'\n",
        "search_keyword = 'PPN 12% since:2024-11-1 until:2024-12-31 lang:id'\n",
        "limit = 5000\n",
        "\n",
        "!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPbSoDDr0z5X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path, delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-YXthxcHTfg"
      },
      "outputs": [],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IujDnqS-vYk9"
      },
      "outputs": [],
      "source": [
        "# @title Mulai gabung data\n",
        "# Install dan import library yang diperlukan\n",
        "!pip install pandas\n",
        "!pip install sastrawi\n",
        "!pip install PySastrawi\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVIkvXT4xrmg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "ppn = pd.read_csv('data.csv')\n",
        "print(ppn.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5VYHx_Yvr_v"
      },
      "outputs": [],
      "source": [
        "ppn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu7q7DlnRQRK"
      },
      "source": [
        "# Explore (Mendefinisikan data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV44GpogwlNr"
      },
      "outputs": [],
      "source": [
        "ppn.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh26Dp4FSFfW"
      },
      "source": [
        "# Modify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyE827YIhNuX"
      },
      "outputs": [],
      "source": [
        "# @title Eliminasi Atribut\n",
        "# membuat file baru yang hanya berisi kolom 'full_text' dan 'labeling'\n",
        "data = ppn[['full_text', 'labeling']]\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j8R5yBSiAzv"
      },
      "outputs": [],
      "source": [
        "# @title Eliminasi Data Label Netral\n",
        "# Menghapus data dengan label 'netral'\n",
        "data = data[data['labeling'] != 'netral']\n",
        "\n",
        "# Reset index setelah menghapus data\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# Menampilkan data hasil filtering\n",
        "#print(data)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cb7MCoBitCo"
      },
      "outputs": [],
      "source": [
        "# @title Remove Duplicates\n",
        "data.drop_duplicates(subset=\"full_text\", keep = 'first', inplace = True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hitung jumlah data untuk setiap label\n",
        "label_counts = data['labeling'].value_counts()\n",
        "labels = label_counts.index\n",
        "counts = label_counts.values\n",
        "percentages = (counts / counts.sum()) * 100\n",
        "total_data = counts.sum()\n",
        "\n",
        "# Buat diagram batang\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(labels, counts, color=['skyblue', 'salmon'])\n",
        "\n",
        "# Tambahkan jumlah & persentase di atas masing-masing batang\n",
        "for bar, count, percentage in zip(bars, counts, percentages):\n",
        "    height = bar.get_height()\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        height + 1,\n",
        "        f'{count} ({percentage:.1f}%)',\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "# Tambahkan judul dan label sumbu\n",
        "plt.title('Jumlah Data per Label Sentimen', fontsize=14)\n",
        "plt.xlabel('Label Sentimen', fontsize=12)\n",
        "plt.ylabel('Jumlah Data', fontsize=12)\n",
        "plt.ylim(0, max(counts) * 1.3)  # beri ruang di atas batang\n",
        "\n",
        "# Tambahkan teks jumlah total data\n",
        "plt.figtext(0.5, 0.8, f'Total Data: {total_data}', ha='center', fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fyjyApl9KAZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH7Gtun6E8lI"
      },
      "outputs": [],
      "source": [
        "# @title Case Folding\n",
        "data['case_folding'] = data['full_text'].str.lower()\n",
        "display(data[['full_text','case_folding']].style.set_sticky())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAdVSvtNU0Tl"
      },
      "outputs": [],
      "source": [
        "# @title Cleaning\n",
        "import string\n",
        "import re\n",
        "\n",
        "def cleaning(tweet):\n",
        "    #remove ascii\n",
        "    tweet = tweet.encode('ascii', 'replace').decode('ascii')\n",
        "    #remove angka\n",
        "    tweet = re.sub('[0-9]+', '', tweet)\n",
        "    #remove RT\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    #remove mention, link, hashtag\n",
        "    tweet = ' '.join(re.sub(\"([@#_][A-Za-z3-9]+)|(\\w+:\\/\\/\\S+)\",\" \", tweet).split())\n",
        "    tweet = re.sub('@[^\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    #remove url\n",
        "    tweet = re.sub(r'\\w+:\\/(Ardy)[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
        "    #remove tanda baca, kecuali tanda %\n",
        "    tweet = re.sub(r'[^\\w\\d\\s]+', '', tweet)\n",
        "    #remove whitespace\n",
        "    tweet = re.sub('\\s+', ' ', tweet)\n",
        "    return tweet\n",
        "data['cleaning'] = data['case_folding'].apply(cleaning)\n",
        "display(data[['case_folding','cleaning']].style.set_sticky())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKKOeMh012fm"
      },
      "outputs": [],
      "source": [
        "# @title Tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "data['tokenize'] = data['cleaning'].apply(word_tokenize_wrapper)\n",
        "display(data[['cleaning','tokenize']].style.set_sticky())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDEeumdoJIBL"
      },
      "outputs": [],
      "source": [
        "# @title Normalize\n",
        "normalized_word = pd.read_excel((\"normalization.xlsx\"), engine='openpyxl')\n",
        "normalized_word_dict = {}\n",
        "for index, row in normalized_word.iterrows():\n",
        "    if row.iloc[0] not in normalized_word_dict:\n",
        "        normalized_word_dict[row.iloc[0]] = row.iloc[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]\n",
        "\n",
        "data['normalize'] = data['tokenize'].apply(normalized_term)\n",
        "display(data[['tokenize','normalize']].style.set_sticky())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XuWuDSiSSv_"
      },
      "outputs": [],
      "source": [
        "#@title Stopword Removal\n",
        "import chardet\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load stopwords bawaan NLTK dan tambahkan stopwords kustom\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "custom_stopwords = {'wkwk', 'jadi', 'menjadi', 'dapat', 'mendapat', 'iya'}\n",
        "stop_words.update(custom_stopwords)\n",
        "\n",
        "# Fungsi menghapus stopwords dari teks\n",
        "def remove_stopwords(tokens): # Changed text to tokens\n",
        "    return [word for word in tokens if word.lower() not in stop_words] # Mengembalikan list token yang sudah dibersihkan\n",
        "\n",
        "data['stopword_removal'] = data['normalize'].apply(remove_stopwords)\n",
        "data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKLXVzccob9l"
      },
      "outputs": [],
      "source": [
        "#@title Stemming\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# function to stem a list of words\n",
        "def stem_words(words):\n",
        "  \"\"\"Stems a list of words, handling potential non-string inputs.\"\"\"\n",
        "  stemmed_words = []\n",
        "  for word in words:\n",
        "    if isinstance(word, str):\n",
        "      stemmed_words.append(stemmer.stem(word))\n",
        "    else:\n",
        "      stemmed_words.append('')  # Or some other appropriate replacement\n",
        "  return stemmed_words\n",
        "\n",
        "# apply stemming to the 'stopword_removal' column\n",
        "data['stemming'] = data['stopword_removal'].apply(stem_words)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus baris dengan nilai kosong di kolom 'stemming'\n",
        "data = data[data['stemming'].notna()]\n",
        "\n",
        "# Menampilkan data setelah penghapusan data kosong\n",
        "print(data)"
      ],
      "metadata": {
        "id": "s4SX-8AX2ij7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('data_stemming.csv', index=False)\n",
        "\n",
        "print(\"File berhasil disimpan sebagai 'hasil_sentimen.csv'\")"
      ],
      "metadata": {
        "id": "zHsnktjb2-y4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecf6a8e-a6cc-42ff-eb7a-9dae349bd888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File berhasil disimpan sebagai 'hasil_sentimen.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKU1HUbkpU7j"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkyQNRhGIKdm"
      },
      "outputs": [],
      "source": [
        "# @title Visualisasi Frekuensi Data (Wordcloud)\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Filter data untuk label positif\n",
        "positive_data = data[data['labeling'] == 'positif']\n",
        "\n",
        "# Gabungkan semua kata dalam kolom 'stemming' untuk label positif\n",
        "all_positive_words = ' '.join(positive_data['stemming'].astype(str).tolist())\n",
        "\n",
        "# Preprocessing tambahan untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    # Hapus karakter non-alfanumerik kecuali spasi\n",
        "    text = re.sub(r\"[^a-zA-Z0-9 ]\", '', text)\n",
        "    # Hapus spasi berlebih\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "all_positive_words = clean_text(all_positive_words)\n",
        "\n",
        "# Hitung frekuensi kata\n",
        "word_counts = Counter(all_positive_words.split())\n",
        "\n",
        "# Ambil 10 kata dengan frekuensi tertinggi\n",
        "top_10_words = word_counts.most_common(10)\n",
        "\n",
        "# Buat DataFrame untuk grafik batang\n",
        "df_top10 = pd.DataFrame(top_10_words, columns=['Kata', 'Frekuensi'])\n",
        "\n",
        "# Buat grafik batang\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(df_top10['Kata'], df_top10['Frekuensi'])\n",
        "plt.title('Top 10 Kata dengan Frekuensi Tertinggi (Label Positif)')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotasi label sumbu x agar mudah dibaca\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pgU-FzLLqcn"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Filter data untuk label positif\n",
        "positive_data = data[data['labeling'] == 'positif']\n",
        "\n",
        "# Gabungkan semua kata dalam kolom 'stemming' untuk label positif\n",
        "all_positive_words = ' '.join(positive_data['stemming'].astype(str).tolist())\n",
        "\n",
        "# Preprocessing tambahan untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    # Hapus karakter non-alfanumerik kecuali spasi dan tanda hubung\n",
        "    text = re.sub(r\"[^a-zA-Z0-9 \\-]\", '', text)  # Mempertahankan tanda hubung\n",
        "    # Hapus spasi berlebih\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "all_positive_words = clean_text(all_positive_words)\n",
        "\n",
        "\n",
        "# Buat objek WordCloud dengan regexp untuk mendeteksi setiap kata\n",
        "wordcloud = WordCloud(width=800, height=400,\n",
        "                      background_color='white',\n",
        "                      stopwords=STOPWORDS,\n",
        "                      regexp=r\"\\w[\\w'-]+\").generate(all_positive_words)\n",
        "\n",
        "# Tampilkan wordcloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Sembunyikan sumbu\n",
        "plt.title('Wordcloud untuk Label Positif')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuX6AJEOL8xq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Filter data untuk label negatif\n",
        "negative_data = data[data['labeling'] == 'negatif']\n",
        "\n",
        "# Gabungkan semua kata dalam kolom 'stemming' untuk label negatif\n",
        "all_negative_words = ' '.join(negative_data['stemming'].astype(str).tolist())\n",
        "\n",
        "# Preprocessing tambahan untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    # Hapus karakter non-alfanumerik kecuali spasi\n",
        "    text = re.sub(r\"[^a-zA-Z0-9 ]\", '', text)\n",
        "    # Hapus spasi berlebih\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "all_negative_words = clean_text(all_negative_words)\n",
        "\n",
        "# Hitung frekuensi kata\n",
        "word_counts = Counter(all_negative_words.split())\n",
        "\n",
        "# Ambil 10 kata dengan frekuensi tertinggi\n",
        "top_10_words = word_counts.most_common(10)\n",
        "\n",
        "# Buat DataFrame untuk grafik batang\n",
        "df_top10 = pd.DataFrame(top_10_words, columns=['Kata', 'Frekuensi'])\n",
        "\n",
        "# Buat grafik batang\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(df_top10['Kata'], df_top10['Frekuensi'])\n",
        "plt.title('Top 10 Kata dengan Frekuensi Tertinggi (Label Negatif)')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotasi label sumbu x agar mudah dibaca\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxbCsj93MdcM"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Filter data untuk label negatif\n",
        "negative_data = data[data['labeling'] == 'negatif']\n",
        "\n",
        "# Gabungkan semua kata dalam kolom 'stemming' untuk label negatif\n",
        "all_negative_words = ' '.join(negative_data['stemming'].astype(str).tolist())\n",
        "\n",
        "# Preprocessing tambahan untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    # Hapus karakter non-alfanumerik kecuali spasi dan tanda hubung\n",
        "    text = re.sub(r\"[^a-zA-Z0-9 \\-]\", '', text)  # Mempertahankan tanda hubung\n",
        "    # Hapus spasi berlebih\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "all_negative_words = clean_text(all_negative_words)\n",
        "\n",
        "\n",
        "# Buat objek WordCloud dengan regexp untuk mendeteksi setiap kata\n",
        "wordcloud = WordCloud(width=800, height=400,\n",
        "                      background_color='white',\n",
        "                      stopwords=STOPWORDS,\n",
        "                      regexp=r\"\\w[\\w'-]+\").generate(all_positive_words)\n",
        "\n",
        "# Tampilkan wordcloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Sembunyikan sumbu\n",
        "plt.title('Wordcloud untuk Label Negatif')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zIrOO4wVOik"
      },
      "outputs": [],
      "source": [
        "# @title Pembobotan Kata (TF-IDF)\n",
        "\n",
        "df=pd.read_csv('data_stemming.csv', usecols=['stemming', 'labeling']).dropna()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "tf = TfidfVectorizer()\n",
        "text_tf = tf.fit_transform(df['stemming'])\n",
        "text_tf\n",
        "\n",
        "temporary_df = pd.DataFrame(text_tf.todense(), columns=tf.get_feature_names_out())\n",
        "temporary_df\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_tf, df['labeling'], test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"\\nHasil proses TF-IDF (DataFrame):\")\n",
        "print(temporary_df)\n",
        "\n",
        "# Hitung rata-rata skor TF-IDF setiap kata di seluruh dokumen\n",
        "average_tfidf = temporary_df.mean().sort_values(ascending=False)\n",
        "\n",
        "# Buat DataFrame untuk kata dengan rata-rata tertinggi\n",
        "average_tfidf_df = pd.DataFrame({\n",
        "    'Kata': average_tfidf.index,\n",
        "    'Rata-rata TF-IDF': average_tfidf.values\n",
        "})\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(\"\\nKata-kata dengan rata-rata TF-IDF tertinggi:\")\n",
        "print(average_tfidf_df.head(20))  # tampilkan 20 kata teratas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS6-0YT5UIX3"
      },
      "outputs": [],
      "source": [
        "# @title Algoritma Naive Bayes (Split Data)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Gabungkan semua teks dalam kolom 'stemming' menjadi satu string per baris\n",
        "data['text_stemming'] = data['stemming'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Buat objek TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Ubah teks 'stemming' menjadi vektor numerik\n",
        "X = vectorizer.fit_transform(data['text_stemming'])\n",
        "y = data['labeling']\n",
        "\n",
        "# Tampilkan info awal\n",
        "print(\"Jumlah total data:\", len(df))\n",
        "print(\"Jumlah data Positif:\", sum(df['labeling'] == 'positif'))\n",
        "print(\"Jumlah data Negatif:\", sum(df['labeling'] == 'negatif'))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Fungsi untuk proses split, pelatihan dan evaluasi\n",
        "def split_and_train(test_ratio, scenario_name):\n",
        "    print(f\"\\n=== Skenario {scenario_name} (test_size={test_ratio}) ===\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        text_tf, df['labeling'], test_size=test_ratio, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"Jumlah Data Latih :\", X_train.shape[0])\n",
        "    print(\"Jumlah Data Uji   :\", X_test.shape[0])\n",
        "\n",
        "    # Hitung jumlah positif dan negatif di data latih\n",
        "    print(\"\\nData Latih:\")\n",
        "    print(\" - Positif:\", sum(y_train == 'positif'))\n",
        "    print(\" - Negatif:\", sum(y_train == 'negatif'))\n",
        "\n",
        "    # Hitung jumlah positif dan negatif di data uji\n",
        "    print(\"\\nData Uji:\")\n",
        "    print(\" - Positif:\", sum(y_test == 'positif'))\n",
        "    print(\" - Negatif:\", sum(y_test == 'negatif'))\n",
        "\n",
        "    # Pelatihan model Naive Bayes\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "# Jalankan untuk 3 skenario\n",
        "split_and_train(0.1, \"90:10\")\n",
        "split_and_train(0.2, \"80:20\")\n",
        "split_and_train(0.3, \"70:30\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Split Data Menggunakan SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Informasi awal\n",
        "print(\"Jumlah total data:\", len(data))\n",
        "print(\"Jumlah data Positif:\", sum(y == 'positif'))\n",
        "print(\"Jumlah data Negatif:\", sum(y == 'negatif'))\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Fungsi split + SMOTE pada data latih\n",
        "def split_and_report_with_smote(test_ratio, scenario_name):\n",
        "    print(f\"\\n=== Skenario {scenario_name} (test_size={test_ratio}) ===\")\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_ratio, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"Sebelum SMOTE (Data Latih):\", Counter(y_train))\n",
        "\n",
        "    # SMOTE pada data latih\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(\"Setelah SMOTE (Data Latih):\", Counter(y_resampled))\n",
        "    print(\"Jumlah Data Latih (Setelah SMOTE):\", X_resampled.shape[0])\n",
        "    print(\"Jumlah Data Uji:\", X_test.shape[0])\n",
        "    print(\" - Positif:\", sum(y_test == 'positif'))\n",
        "    print(\" - Negatif:\", sum(y_test == 'negatif'))\n",
        "\n",
        "    return X_resampled, y_resampled, X_test, y_test\n",
        "\n",
        "# Jalankan 3 skenario\n",
        "X90, y90, X90_test, y90_test = split_and_report_with_smote(0.1, \"90:10\")\n",
        "X80, y80, X80_test, y80_test = split_and_report_with_smote(0.2, \"80:20\")\n",
        "X70, y70, X70_test, y70_test = split_and_report_with_smote(0.3, \"70:30\")"
      ],
      "metadata": {
        "id": "2zfJA6uiv3s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw85SXpuO94x"
      },
      "source": [
        "# Assess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5j2TekFZMsJ"
      },
      "outputs": [],
      "source": [
        "# @title Confusion Matrix\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report)\n",
        "# Gabungkan semua teks dalam kolom 'stemming' menjadi satu string per baris\n",
        "data['text_stemming'] = data['stemming'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Buat objek TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 3. Skenario Pembagian Data\n",
        "scenarios = [\n",
        "    (0.1, \"90:10\"),\n",
        "    (0.2, \"80:20\"),\n",
        "    (0.3, \"70:30\")\n",
        "]\n",
        "\n",
        "# 4. Evaluasi Model\n",
        "for test_size, scenario_name in scenarios:\n",
        "    # ========== TANPA SMOTE ==========\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(f\"## [TANPA SMOTE] Skenario: {scenario_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ========== DENGAN SMOTE ==========\n",
        "    # SMOTE hanya diterapkan pada data latih\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    model_smote = MultinomialNB()\n",
        "    model_smote.fit(X_train_smote, y_train_smote)\n",
        "    y_pred_smote = model_smote.predict(X_test)\n",
        "\n",
        "    print(f\"## [DENGAN SMOTE] Skenario: {scenario_name}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_smote, zero_division=0))\n",
        "    print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}